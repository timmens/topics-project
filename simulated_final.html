

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Final &#8212; Topics in Econometrics and Statistics</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/mystnb.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Stock Data" href="stock.html" />
    <link rel="prev" title="Reverse Engineering" href="simulated_reverse_engineering.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  
  <h1 class="site-logo" id="site-title">Topics in Econometrics and Statistics</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active">
  <a class="reference internal" href="simulated.html">
   Simulated Data
  </a>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="simulated_intro.html">
     Data Description
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="simulated_reverse_engineering.html">
     Reverse Engineering
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Final
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="stock.html">
   Stock Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="zreferences.html">
   Bibliography
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/simulated_final.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preliminaries">
   Preliminaries
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-model-using-a-subset-of-features">
   1. Linear Model Using a Subset of Features
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-neural-network-with-dropout-regularization">
   2. Deep Neural Network with Dropout Regularization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#two-stage-random-forest-with-feature-selection">
   3. Two-stage Random Forest with Feature Selection
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-tree-boosting-the-final-model">
   4. Gradient tree boosting (
   <em>
    the final model
   </em>
   )
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#theory">
     Theory
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementation-application">
     Implementation / Application
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mse-comparison">
   MSE Comparison
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="final">
<h1>Final<a class="headerlink" href="#final" title="Permalink to this headline">¶</a></h1>
<p>In this last section I will list my top four models and their respective performances on a validation set. However, only for my final model I will explain how the fitting procedure works in detail.</p>
<p>In particular I consider a</p>
<ol class="simple">
<li><p>Linear model using a subset of features</p></li>
<li><p>Deep neural network with dropout regularization</p></li>
<li><p>Two-stage random forest with feature selection</p></li>
<li><p>Gradient tree boosting (<em>the final model</em>)</p></li>
</ol>
<p>If you only care about the final model please jump directly to subsection 4, in which I explain how the model is fit on a theoretical basis as well as the (Python) implementation. Note also that in the very last subsection I include a table comparing the validation mean squared error for all of the presented models.</p>
<div class="section" id="preliminaries">
<h2>Preliminaries<a class="headerlink" href="#preliminaries" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dropout</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.wrappers.scikit_learn</span> <span class="kn">import</span> <span class="n">KerasRegressor</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="kn">from</span> <span class="nn">catboost</span> <span class="kn">import</span> <span class="n">CatBoostRegressor</span>

<span class="n">ROOT</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">())</span><span class="o">.</span><span class="n">parent</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">df_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">ROOT</span> <span class="o">/</span> <span class="s2">&quot;bld&quot;</span> <span class="o">/</span> <span class="s2">&quot;train_simulated.parquet&quot;</span><span class="p">)</span>
<span class="n">df_val</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">ROOT</span> <span class="o">/</span> <span class="s2">&quot;bld&quot;</span> <span class="o">/</span> <span class="s2">&quot;validate_simulated.parquet&quot;</span><span class="p">)</span>

<span class="n">y_train</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">[</span><span class="s2">&quot;Y&quot;</span><span class="p">]</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">df_train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;Y&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">y_val</span> <span class="o">=</span> <span class="n">df_val</span><span class="p">[</span><span class="s2">&quot;Y&quot;</span><span class="p">]</span>
<span class="n">X_val</span> <span class="o">=</span> <span class="n">df_val</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;Y&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="linear-model-using-a-subset-of-features">
<h2>1. Linear Model Using a Subset of Features<a class="headerlink" href="#linear-model-using-a-subset-of-features" title="Permalink to this headline">¶</a></h2>
<p>Here I fit a simple “two-step” 3rd degree polynomial regression model to benchmark the machine learning methods from below. The procedure has two steps as I only consider the features <span class="math notranslate nohighlight">\(\{X_3, X_{12}, X_{38}\}\)</span>. This set of features was selected in the previous section using the recursive feature elimination strategy. However, basically all other feature selection strategies considered in the previous section selected a similar set of features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">relevant</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;X</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">38</span><span class="p">]]</span>

<span class="n">XX_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">relevant</span><span class="p">]</span>
<span class="n">XX_val</span> <span class="o">=</span> <span class="n">X_val</span><span class="p">[</span><span class="n">relevant</span><span class="p">]</span>

<span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">XX_train</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">XX_train</span><span class="p">)</span>
<span class="n">XX_val</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">XX_val</span><span class="p">)</span>

<span class="n">lm</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">lm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">XX_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">prediction</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">XX_val</span><span class="p">)</span>
<span class="n">mse_lm</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;(Linear Model) MSE: </span><span class="si">{</span><span class="n">mse_lm</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>(Linear Model) MSE: 5.361621926612066
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="deep-neural-network-with-dropout-regularization">
<h2>2. Deep Neural Network with Dropout Regularization<a class="headerlink" href="#deep-neural-network-with-dropout-regularization" title="Permalink to this headline">¶</a></h2>
<p>I fit a deep neural network using the popular <a class="reference external" href="https://keras.io/">keras</a> library (<a class="bibtex reference internal" href="zreferences.html#keras" id="id1">[C+15]</a>) which provides an intuitive API for the powerful <a class="reference external" href="https://www.tensorflow.org/">tensorflow</a> package (<a class="bibtex reference internal" href="zreferences.html#tensorflow" id="id2">[AAB+15]</a>). The neural network architecture is set using the <code class="docutils literal notranslate"><span class="pre">build_regressor</span></code> function. I choose an archtecture with 7 layers. For the first layer I choose 50 hidden nodes, 25 hidden nodes for the second layer and 10 hidden nodes for the third to seventh layer. Moreover I use the so called <a class="reference external" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLu</a> activation function, which has been proven to outperform the classic sigmoid activation function in several ways, see for example <a class="bibtex reference internal" href="zreferences.html#krizhevsky2017" id="id3">[KSH17]</a>. As overfitting is a big problem with deep networks I employ a popular technique called dropout regularization to mitigate this effect, see <a class="bibtex reference internal" href="zreferences.html#srivastava2014" id="id4">[SHK+14]</a>. Dropout leads to neurons in a layer being randomly deactivated for a single epoch during the backpropagation, which in turn leads to neighboring neurons not developing a relationship that is too strongly dependent, which in turn is said to mitigate overfitting.</p>
<p><em><strong>Note.</strong></em></p>
<p>I decide to use this specific architecture as I “learned” from the previous section that the main effects are sparse in the sense that only very few features are relevant. However, I also realized from playing around with some parameters and the validation error of the linear model that some effects must be highly non-linear. An architecture which reduces the nodes from the original 100 input dimensions to 50, then 25 and then 10 forces the network to select the relevant features. And by using the other additional 5 layers the network can find non-linear signals.</p>
<p><em><strong>Remark.</strong></em></p>
<p>Since the gradient boosted tree presented below performs so well I did not consider many different architectures. I do believe that the neural networks should be able to perform comparably well if a different architecture is chosen.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_COL</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="k">def</span> <span class="nf">build_regressor</span><span class="p">():</span>
    <span class="n">regressor</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    <span class="c1"># first hidden layer</span>
    <span class="n">regressor</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">N_COL</span><span class="p">))</span>
    <span class="n">regressor</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
    
    <span class="c1"># second hidden layer</span>
    <span class="n">regressor</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>
    <span class="n">regressor</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
    
    <span class="c1"># third to tenth hidden layer</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="n">regressor</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>
    
    <span class="c1"># output layer</span>
    <span class="n">regressor</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">))</span>
    
    <span class="c1"># compile model</span>
    <span class="n">regressor</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;adam&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mean_squared_error&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">regressor</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">nnet</span> <span class="o">=</span> <span class="n">KerasRegressor</span><span class="p">(</span>
    <span class="n">build_fn</span><span class="o">=</span><span class="n">build_regressor</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
<span class="n">nnet</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">prediction</span> <span class="o">=</span> <span class="n">nnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">mse_nnet</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;(Neural Network) MSE: </span><span class="si">{</span><span class="n">mse_nnet</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>(Neural Network) MSE: 5.369156156602011
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="two-stage-random-forest-with-feature-selection">
<h2>3. Two-stage Random Forest with Feature Selection<a class="headerlink" href="#two-stage-random-forest-with-feature-selection" title="Permalink to this headline">¶</a></h2>
<p>In this two stage procedure I first fit a random forest on the full set of features. I then consider a feature importance measure, which can be automatically calculated from the random forest fit. Using this I select the 30 <em>most</em> important features and fit another random forest on this subset of features.</p>
<p><em><strong>First Stage</strong></em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> 
    <span class="n">max_features</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> 
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span>
    <span class="p">[</span><span class="n">tree</span><span class="o">.</span><span class="n">feature_importances_</span> <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="n">rf</span><span class="o">.</span><span class="n">estimators_</span><span class="p">],</span> 
    <span class="n">axis</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">rf</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">relevant</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;X</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">[:</span><span class="mi">30</span><span class="p">]]</span>

<span class="n">prediction</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">mse_first_stage</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;First stage MSE: </span><span class="si">{</span><span class="n">mse_first_stage</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>First stage MSE: 5.1615609346407565
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># plotting code: can be safely ignored</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;font.size&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span> <span class="s1">&#39;figure.figsize&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">)})</span>
<span class="n">cut</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">x</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])[:</span><span class="n">cut</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">[</span><span class="n">indices</span><span class="p">][:</span><span class="n">cut</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Feature importances (displaying only the </span><span class="si">{</span><span class="n">cut</span><span class="si">}</span><span class="s2"> most important features)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">std</span><span class="p">[</span><span class="n">indices</span><span class="p">][:</span><span class="n">cut</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">[:</span><span class="n">cut</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">cut</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/simulated_final_11_0.png" src="_images/simulated_final_11_0.png" />
</div>
</div>
<p><em><strong>Second Stage</strong></em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">XX_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">relevant</span><span class="p">]</span>
<span class="n">XX_val</span> <span class="o">=</span> <span class="n">X_val</span><span class="p">[</span><span class="n">relevant</span><span class="p">]</span>

<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> 
    <span class="n">max_features</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> 
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">XX_train</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="n">prediction</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">XX_val</span><span class="p">)</span>
<span class="n">mse_rf</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;(Random Forest) MSE: </span><span class="si">{</span><span class="n">mse_rf</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>(Random Forest) MSE: 5.120511556417796
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="gradient-tree-boosting-the-final-model">
<h2>4. Gradient tree boosting (<em>the final model</em>)<a class="headerlink" href="#gradient-tree-boosting-the-final-model" title="Permalink to this headline">¶</a></h2>
<p>The final model I am using is a specific variant of a gradient boosted tree. The key concepts of this method are equivalent to the seminal paper by Friedman, see <a class="bibtex reference internal" href="zreferences.html#friedman2002" id="id5">[Fri02]</a>. The version I am using is implemented in the <a class="reference external" href="https://catboost.ai/">catboost</a> package, which differs slightly from common other implementations. Next I will introduce the concept of gradient boosting with a particular focus on tree weak-learners. Afterwards I show how to fit a model using <code class="docutils literal notranslate"><span class="pre">catboost</span></code>. Note that my final prediction submissions are not made with this specific model since here I only use my training sample. The final predictions are made with the script <a class="reference external" href="https://github.com/timmens/topics-project/blob/main/codes/final_prediction.py">final_prediction.py</a>.</p>
<div class="section" id="theory">
<h3>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h3>
<p>I will first explain the general idea behind boosting and gradient boosting. Then I show how the general formulas simplify when using (regression) trees as base-learners. At last I illustrate how out-of-sample prediction loss may be improved by using ideas from stochastic gradient descent and regularization.</p>
<p><strong>Note.</strong> My notation and explaination is guided by <a class="bibtex reference internal" href="zreferences.html#friedman2002" id="id6">[Fri02]</a> and <a class="bibtex reference internal" href="zreferences.html#esl2001" id="id7">[HTF01]</a>.</p>
<hr class="docutils" />
<p><strong>Notation and Preliminaries</strong></p>
<p>Assume we are given a data set <span class="math notranslate nohighlight">\(\{(x_i, y_i) : i=1,\dots,N\}\)</span>, with <span class="math notranslate nohighlight">\(x_i \in \mathbb{R}^p\)</span> and <span class="math notranslate nohighlight">\(y_i \in \mathbb{R}\)</span>. These observations are assumed to be i.i.d. according to some joint distribution <span class="math notranslate nohighlight">\(\mathbb{P}_{xy}\)</span>. A goal of statistical learning is to find a function <span class="math notranslate nohighlight">\(f^* : \mathbb{R}^p \to \mathbb{R}\)</span> such that
\begin{align*}
f^* = \underset{f}{\text{argmin}} , \mathbb{E}_{xy}\left[L(y, f(x)) \right] ,,
\end{align*}
given some loss function <span class="math notranslate nohighlight">\(L\)</span>, where the expecation is taken over the joint distribution of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> values.</p>
<p>As is very common in statistical learning, boosting is a procedure to approximate <span class="math notranslate nohighlight">\(f^*\)</span> by an additive model of the form</p>
<p>\begin{align}
f(x) = \sum_{m=0}^M f_m(x) = \sum_{m=0}^M \beta_m b(x; \gamma_m) ,,
\end{align}
where the <span class="math notranslate nohighlight">\(\beta_m\)</span> denote expansion coefficients and <span class="math notranslate nohighlight">\(b(\cdot\,,\gamma) : \mathbb{R}^p \to \mathbb{R}\)</span> denote the so called <em>base-learners</em> which are parameterized by <span class="math notranslate nohighlight">\(\gamma\)</span>. If feasible in general we would like to estimate the parameters by solving</p>
<p>\begin{align}
\underset{{\beta_m, \gamma_m}<em>1^M}{\min} , \sum</em>{i=1}^N L\left(y_i, \sum_{m=0}^M \beta_m b(x_i; \gamma_m)\right) ,.
\end{align}</p>
<p>For a general loss function and base-learner, however, this optimization is computationally intractable.</p>
<hr class="docutils" />
<p><strong>General Concept</strong></p>
<p>A simple algorithm to approximate the coefficient estimates from above is to fit each tuple <span class="math notranslate nohighlight">\((\beta_m, \gamma_m)\)</span> in a stage-wise fashion. That is, one starts with an initial guess <span class="math notranslate nohighlight">\(f_0\)</span> and then</p>
<p>For each <span class="math notranslate nohighlight">\(m=1,\dots, M\)</span>:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\((\beta_m, \gamma_m) = \underset{\beta, \gamma}{\text{argmin}} \, \sum_{i=1}^N L \left(y_i, f_{m-1}(x_i) + \beta b(x_i; \gamma) \right)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f_m(x) = f_{m-1}(x) + \beta_m b(x, \gamma_m)\)</span>.</p></li>
</ol>
<p>This simplifies the optimization problem from above consideribly, but again, for general loss functions and base-learners step 1 can be hard to solve.</p>
<hr class="docutils" />
<p><strong>Gradient Boosting</strong></p>
<p>Gradient boosting, as proposed in <a class="bibtex reference internal" href="zreferences.html#friedman2000" id="id8">[Fri00]</a>, approximately solves the first step from above for differentiable loss functions using a three step procedure. In the <span class="math notranslate nohighlight">\(m\)</span>-th step from above we do</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(r_{im} := - \left[\frac{\partial L(y_i, f(x_i))}{\partial f(x_i)}  \right]_{f(x) = f_{m-1}(x)}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma_m = \underset{\gamma, \rho}{\text{argmin}} \, \sum_{i=1}^N \left(r_{im} - \rho b(x_i; \gamma) \right)^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_m = \underset{\beta}{\text{argmin}} \, \sum_{i=1}^N L \left(y_i, f_{m-1}(x_i) + \beta b(x_i; \gamma_m) \right)\)</span>.</p></li>
</ol>
<p>That is, we first compute the gradient, whose entries are called <em>pseudo-residuals</em>, and fit a single base-learner using the least-squares method. Lastly we have to solve a general optimization problem, but only in a single variable.</p>
<p>With squared-error loss <span class="math notranslate nohighlight">\(L(y, x) = \frac{1}{2}(y - x)^2\)</span> the pseudo-residuals <span class="math notranslate nohighlight">\(r_{im}\)</span> are equivalent to the actual residuals <span class="math notranslate nohighlight">\(r_{im} = y_i - f_{m-1}(x_i)\)</span>. This allows for the nice interpretation of the procedure that given any step <span class="math notranslate nohighlight">\(m\)</span>, we consider the deviations of the labels <span class="math notranslate nohighlight">\(y_i\)</span> from our current estimate <span class="math notranslate nohighlight">\(f_{m-1}(x_i)\)</span> and train a model to fit these deviations. With other loss functions, say absolute-error loss <span class="math notranslate nohighlight">\(L(y, x) = |y - x|\)</span>, we get <span class="math notranslate nohighlight">\(r_{im} = \text{sign}(y_i - f_{m-1}(x_i))\)</span> and in the <span class="math notranslate nohighlight">\(m\)</span>-th step the base-learner is fit to simply predict the direction to which a sample deviates, which is more robust to outliers as it ignores the magnitude of the deviation.</p>
<hr class="docutils" />
<p><strong>Gradient Tree Boosting</strong></p>
<p>A popular choice of base-learners are decision trees. When using trees as base learners some steps in the generic algorithm simplify and some can even be improved. First, let us formally define a tree. A regression tree with <span class="math notranslate nohighlight">\(J\)</span> terminal-nodes is a function
\begin{align}
T\left(x; {\alpha_j, R_j}<em>{j=1}^J \right) = \sum</em>{j=1}^J \alpha_j \mathbb{1}_{R_j} (x) ,,
\end{align}
where <span class="math notranslate nohighlight">\(\{R_j\}_{j=1}^J\)</span> is a partition of the feature space <span class="math notranslate nohighlight">\(\mathbb{R}^p\)</span>. Strictly speaking, <span class="math notranslate nohighlight">\(J\)</span> is also a parameter, however, it is usually considered a <em>hyper-parameter</em> which has to be chosen using prior information or via methods like cross-validation.</p>
<p>Let us now consider the second step of the above algorithm. First note that optimizing over <span class="math notranslate nohighlight">\(\rho\)</span> is irrelevant in the case of trees as we can always define <span class="math notranslate nohighlight">\(\tilde{\alpha}_j := \rho \alpha_j\)</span>. But then solving 2 is equivalent to solving</p>
<p>\begin{align}
{\alpha_{jm}, R_{jm}}<em>{j=1}^J =: \gamma_m = \underset{\gamma}{\text{argmin}} , \sum</em>{i=1}^N \left(r_{im} - T(x_i; \gamma) \right)^2 ,,
\end{align}
For a given <span class="math notranslate nohighlight">\(J\)</span> this combinatorial optimization problem is again computationally intractable, but there are many algorithms to solve it approximately; see for example the CART algorithm in <a class="bibtex reference internal" href="zreferences.html#esl2001" id="id9">[HTF01]</a>.</p>
<p>Henceforth say we have (approximately) solved the tree optimization problem (step 2) and are left to optimize for the constant <span class="math notranslate nohighlight">\(\beta_m\)</span> (step 3). As will be seen, with trees we can even go one step further and choose an optimal value for each terminal-node region. We  assume we are given parameters <span class="math notranslate nohighlight">\(\gamma_m = \{\alpha_j, R_j\}_{j=1}^J\)</span>. The next simplification when using trees stems from the fact the the <span class="math notranslate nohighlight">\(R_j\)</span>’s form a partition of the feature space. Instead of step 3 we can thus write</p>
<p>\begin{align}
{\beta_{jm}}<em>{j=1}^J = \underset{\beta_1,\dots,\beta_J}{\text{argmin}} , \sum</em>{j=1}^J \sum_{x_i \in R_j} L \left(y_i, f_{m-1}(x_i) + \beta_j \right) ,,
\end{align}
which can be solved for each region seperately. Note that we can write <span class="math notranslate nohighlight">\(L \left(y_i, f_{m-1}(x_i) + \beta_j \right)\)</span> instead of <span class="math notranslate nohighlight">\(L \left(y_i, f_{m-1}(x_i) + \beta_j \alpha_j \right)\)</span> for the same reasons as stated above. For example with squared error loss we would then get <span class="math notranslate nohighlight">\(\beta_{jm} = \text{mean}(y_i - f_{m-1}(x_i) : x_i \in R_{jm}) = \text{mean}(r_{im} : x_i \in R_{jm})\)</span>.</p>
<hr class="docutils" />
<p><strong>Algorithm</strong></p>
<p>For the sake of clarity I illustrate the complete gradient tree boosting algorithm. This corresponds to Algorithm 10.3 (Gradient Tree Boosting Algorithm) in <a class="bibtex reference internal" href="zreferences.html#esl2001" id="id10">[HTF01]</a>.</p>
<hr class="docutils" />
<p><em>Input</em>: <span class="math notranslate nohighlight">\(M\)</span> (number of trees), <span class="math notranslate nohighlight">\(J\)</span> (number of terminal nodes), <span class="math notranslate nohighlight">\(L\)</span> loss function, <span class="math notranslate nohighlight">\(\{(x_i, y_i)\}_{i=1}^N\)</span> training data.</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(f_0 = \underset{\gamma}{\text{argmin}} \sum_{i=1}^N L(y_i, \gamma)\)</span></p></li>
<li><p>For <span class="math notranslate nohighlight">\(m = 1, \dots, M\)</span>:</p>
<ul class="simple">
<li><p>a) For <span class="math notranslate nohighlight">\(i=1,\dots,N\)</span> compute <span class="math notranslate nohighlight">\(r_{im} := - \left[\frac{\partial L(y_i, f(x_i))}{\partial f(x_i)}  \right]_{f = f_{m-1}}\)</span></p></li>
<li><p>b) Fit a tree to <span class="math notranslate nohighlight">\(\{(x_i, r_{im})\}_{i=1}^N\)</span> resulting in regions <span class="math notranslate nohighlight">\(\{R_{jm}\}_{j=1}^J\)</span></p></li>
<li><p>c) For <span class="math notranslate nohighlight">\(j=1,\dots,J\)</span> solve <span class="math notranslate nohighlight">\(\gamma_{jm} = \underset{\gamma}{\text{argmin}} \sum_{x_i \in R_{jm}} L(y_i, f_{m-1}(x_i) + \gamma)\)</span></p></li>
<li><p>d) Update <span class="math notranslate nohighlight">\(f_m = f_{m-1} + \sum_{j=1}^J \gamma_{jm} \mathbb{1}_{R_{jm}}\)</span></p></li>
</ul>
</li>
<li><p>Return <span class="math notranslate nohighlight">\(f = f_M\)</span></p></li>
</ol>
<hr class="docutils" />
<p><strong>Enhancements</strong></p>
<p>Until now we have not mentioned two very important concepts used widely in the statistical and machine learning literature. Injecting randomness, which is used to decorrelate trees and avoid overfitting, and regularization, which is used to avoid overfitting and thus improve out-of-sample loss.</p>
<ul class="simple">
<li><p><em><strong>Injecting Randomness.</strong></em>
From empirical experience it has been seen that methods like bagging <a class="bibtex reference internal" href="zreferences.html#breiman96" id="id11">[Bre96]</a> and random forests <a class="bibtex reference internal" href="zreferences.html#breiman2001" id="id12">[Bre01]</a> outperform similar algorithms that do not explicitly make use of randomness. Similarly stochastic gradient descent is considered superior to the classical gradient descent algorithm when used in a machine learning setting; see <a class="bibtex reference internal" href="zreferences.html#bottou2016" id="id13">[BCN16]</a>. The easiest way to inject randomness to the gradient boosting algorithm is described in <a class="bibtex reference internal" href="zreferences.html#friedman2002" id="id14">[Fri02]</a>. In the second step of the above algorithm we consider a random subset of the data of size <span class="math notranslate nohighlight">\(N' &lt; N\)</span>. On top of potentially improving out-of-sample fit, this can lead to significantly shorter training times.</p></li>
<li><p><em><strong>Regularization.</strong></em>
Similarly as injecting randomness, a popular technique to avoid overfitting is regularization. Especially shrinkage methods like ridge regression and the lasso have gained immense popularity; see for example <a class="bibtex reference internal" href="zreferences.html#slws2015" id="id15">[HTW15]</a>. A immediate enhancement to the above algorithm is to include a shrinkage paramater <span class="math notranslate nohighlight">\(0 &lt; \nu \leq 1\)</span>, which is usually called the <em>learning rate</em>. Then, in step 2.d) we modify the equation slightly to get
<span class="math notranslate nohighlight">\($f_m = f_{m-1} + \nu \sum_{j=1}^J \gamma_{jm} \mathbb{1}_{R_{jm}} \,.\)</span>$
It was found empirically that small values of the learning rate, <span class="math notranslate nohighlight">\(\nu &lt; 0.1\)</span>, lead to better generalization; see <a class="bibtex reference internal" href="zreferences.html#friedman2000" id="id16">[Fri00]</a>.</p></li>
</ul>
<hr class="docutils" />
<p><strong>Catboost</strong></p>
<p>Differences:</p>
<ul class="simple">
<li><p>Unbiased Gradients</p></li>
<li><p>Oblivious Trees</p></li>
<li><p>Categorical Features</p></li>
</ul>
</div>
<div class="section" id="implementation-application">
<h3>Implementation / Application<a class="headerlink" href="#implementation-application" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">gbt</span> <span class="o">=</span> <span class="n">CatBoostRegressor</span><span class="p">(</span>
    <span class="n">iterations</span><span class="o">=</span><span class="mi">1500</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> 
    <span class="n">depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">loss_function</span><span class="o">=</span><span class="s2">&quot;RMSE&quot;</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">gbt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">prediction</span> <span class="o">=</span> <span class="n">gbt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">mse_gbt</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;(Catboost) MSE: </span><span class="si">{</span><span class="n">mse_gbt</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-none notranslate"><div class="highlight"><pre><span></span>(Catboost) MSE: 5.048105271497743
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="mse-comparison">
<h2>MSE Comparison<a class="headerlink" href="#mse-comparison" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;nnet&quot;</span><span class="p">,</span> <span class="s2">&quot;lm&quot;</span><span class="p">,</span> <span class="s2">&quot;rf&quot;</span><span class="p">,</span> <span class="s2">&quot;catboost&quot;</span><span class="p">]</span>
<span class="n">mses</span> <span class="o">=</span> <span class="p">[</span><span class="n">mse_nnet</span><span class="p">,</span> <span class="n">mse_lm</span><span class="p">,</span> <span class="n">mse_rf</span><span class="p">,</span> <span class="n">mse_gbt</span><span class="p">]</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">mses</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">,</span> <span class="s2">&quot;mse&quot;</span><span class="p">])</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/simulated_final_19_0.png" src="_images/simulated_final_19_0.png" />
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="simulated_reverse_engineering.html" title="previous page">Reverse Engineering</a>
    <a class='right-next' id="next-link" href="stock.html" title="next page">Stock Data</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Tim Mensinger<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="_static/js/index.js"></script>
    
  </body>
</html>