{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final\n",
    "\n",
    "My final prediction model is build on the transformed data set from the previous section. That is, I ignore that the time index contains special information and act as if it were only another features. In the following I consider three models.\n",
    "\n",
    "1. Linear model\n",
    "2. Two-stage linear model (*the final model*)\n",
    "\n",
    "Again, if you only care about the final model please jump directly to subsection 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "ROOT = Path(os.getcwd()).parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(ROOT / \"bld\" / \"train_stock.parquet\")\n",
    "df_val = pd.read_parquet(ROOT / \"bld\" / \"validate_stock.parquet\")\n",
    "\n",
    "y_train = df_train[\"Y\"]\n",
    "X_train = df_train.drop(\"Y\", axis=1)\n",
    "\n",
    "y_val = df_val[\"Y\"]\n",
    "X_val = df_val.drop(\"Y\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Model\n",
    "\n",
    "Here I fit a simple unregularized linear model which is used as a lower benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Linear Model) MSE: 0.9524997479334116\n"
     ]
    }
   ],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "prediction = lm.predict(X_val)\n",
    "mse_lm = mean_squared_error(y_val, prediction)\n",
    "print(f\"(Linear Model) MSE: {mse_lm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Two-stage Linear Model (*final model*)\n",
    "\n",
    "To mix things up, here I select features using a Lasso approach. With these features I then fit a simple 2nd degree polynomial model. The code which I used to construct the final predictions can be found in the script [final_prediction.py](https://github.com/timmens/topics-project/blob/main/codes/final_prediction.py).\n",
    "\n",
    "***Lasso feature selection***\n",
    "\n",
    "The regularization parameter is selected via a 5-fold cross-validation procedure over a logspace grid (a sequence which is linear on a logarithmic scale). I select all columns which have nonzero coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant features chosen via Lasso:\n",
      "['at_adj', 'beme', 'cum_return_12_2', 'cum_return_12_7', 'cum_return_1_0', 'cum_return_36_13', 'd_so', 'e2p', 'free_cf', 'noa', 'pcm', 'pm', 'pm_adj', 'ret_max', 'suv', 'year']\n"
     ]
    }
   ],
   "source": [
    "lasso_model = LassoCV(alphas=np.logspace(-2.5, 1, 50), cv=5)\n",
    "lasso_model = lasso_model.fit(X_train, y_train)\n",
    "\n",
    "relevant = X_train.columns[lasso_model.coef_ != 0].to_list()\n",
    "print(\"Relevant features chosen via Lasso:\")\n",
    "print(relevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Polynomial regression on subset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(X, relevant_columns):\n",
    "    \"\"\"Return 2nd degree polynomial features plus third power.\"\"\"\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    XX = poly.fit_transform(X[relevant_columns])\n",
    "    XX = np.concatenate((XX, X ** 3), axis=1)\n",
    "    return XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_train = make_features(X_train, relevant)\n",
    "XX_val = make_features(X_val, relevant)\n",
    "\n",
    "pm = LinearRegression()\n",
    "pm.fit(XX_train, y_train)\n",
    "del XX_train # memory ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Lasso Polynomial Model) MSE: 0.9452907741665161\n"
     ]
    }
   ],
   "source": [
    "predictions = pm.predict(XX_val)\n",
    "mse_pm = mean_squared_error(y_val, predictions)\n",
    "print(f\"(Lasso Polynomial Model) MSE: {mse_pm}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
